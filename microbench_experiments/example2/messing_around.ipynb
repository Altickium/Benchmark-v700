{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data framework: the basic paradigm\n",
    "\n",
    "user implements one function define_experiment\n",
    "\n",
    "then runs run_experiments.py\n",
    "\n",
    "it runs potentially many experimental trials (over all defined configurations), captures output, builds a sqlite database, queries it, produces plots, produces html pages to display plots...\n",
    "\n",
    "also lots of tools to do querying, plot generation and analysis in jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the following code cell before any others\n",
    "\n",
    "It does basic initialization for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "print(\"Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 'hello world' of `run_experiments.sh`\n",
    "\n",
    "defining a trivial experiment that compiles and runs a single command once and saves the output.\n",
    "\n",
    "we do run_in_jupyter and pass define_experiment. could save define_experiment in a python file and run the equivalent run_experiments.sh command..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')     ## working dir for compiling\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin') ## working dir for running\n",
    "    set_cmd_compile  (exp_dict, 'make -j all')\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./ubench_brown_ext_abtree_lf.alloc_new.reclaim_debra.pool_none.out -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 3000')\n",
    "\n",
    "run_in_jupyter(define_experiment, cmdline_args='--production')\n",
    "# assuming that the define_experiment() function above is saved in myexp.py,\n",
    "# the run_in_jupyter line above is equivalent to running:\n",
    "#   ../../tools/data_framework/run_experiments.sh myexp.py --production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Re)running results without compiling\n",
    "\n",
    "we will remedy the warning message soon...\n",
    "\n",
    "introduce rerunning experiments without compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')     ## working dir for compiling\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin') ## working dir for running\n",
    "    set_cmd_compile  (exp_dict, 'make -j all')\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../lib/libjemalloc.so numactl --interleave=all time ./ubench_brown_ext_abtree_lf.alloc_new.reclaim_debra.pool_none.out -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 3000')\n",
    "\n",
    "run_in_jupyter(define_experiment, cmdline_args='--production --no-compile')\n",
    "# equiv: [...]/run_experiments.sh myexp.py --production --no-compile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data files (captured stdout/err)\n",
    "\n",
    "introduce concept of data files (capturing the output of the run)\n",
    "\n",
    "this is the output of that one run command we did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(shell_to_str('cat data/data000001.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running with varying parameters\n",
    "\n",
    "of course running one command isn't very interesting...\n",
    "\n",
    "introduce run params\n",
    "\n",
    "    ## add parameters that you want your experiments to be run with.\n",
    "    ## your program will be run once for each set of values in the CROSS PRODUCT of all parameters.\n",
    "    ## (i.e., we will run your program with every combination of parameters)\n",
    "\n",
    "introduce replacement strings: {DS_TYPENAME}\n",
    "\n",
    "    ## you can use any of the run params you define to dynamically replace {_tokens_like_this} in strings. for example, we can include {DS_TYPENAME} in our run command, and it will be replaced by the current value of {DS_TYPENAME} (that's right, we can run different commands based on the current value of DS_TYPENAME)\n",
    "    ## you can also get the paths to key directories by using:\n",
    "    ##      {__dir_compile}\n",
    "    ##      {__dir_run}\n",
    "    ##      {__dir_data}\n",
    "    ##\n",
    "    ## the following replacement token is also defined for you:\n",
    "    ##      {__step}            the number of runs done so far, padded to six digits with leading zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j all')\n",
    "\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../../lib/libjemalloc.so numactl --interleave=all time ./ubench_{DS_TYPENAME}.alloc_new.reclaim_debra.pool_none.out -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 3000')\n",
    "\n",
    "run_in_jupyter(define_experiment, cmdline_args='--production --no-compile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data fields from captured stdout/err\n",
    "\n",
    "note 3 data files were produced this time... one for each value of `DS_TYPENAME`. let's put those data files to use and get rid of that warning, by specifying that we want to *extract* some text from each data file.\n",
    "\n",
    "in particular, let's extract a line of the form \"`DS_TYPENAME=...`\" and a line of the form \"`total_throughput=...`\" from each data file. (you can find such lines in the data file above if you like.)\n",
    "\n",
    "extracted data is stored in a sqlite database `data/output_database.sqlite` in a table called `data`. (each field name passed to `add_data_field` becomes a **column** in `data`.)\n",
    "\n",
    "to do this, we call `add_data_field()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j all')\n",
    "\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../../lib/libjemalloc.so numactl --interleave=all time ./ubench_{DS_TYPENAME}.alloc_new.reclaim_debra.pool_none.out -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 3000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'DS_TYPENAME')\n",
    "    add_data_field   (exp_dict, 'total_throughput')\n",
    "\n",
    "run_in_jupyter(define_experiment, cmdline_args='--production --no-compile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying the database\n",
    "\n",
    "Note that we can simply **access** the last database we created, *WITHOUT rerunning* any experiments, by adding `--no-run --no-createdb` to `cmdline_args` in our `run_in_jupyter` call.\n",
    "\n",
    "Also note that you can accomplish the same thing from the **command line** by running `../../tools/data_framework/run_experiments.py` with the **same** `cmdline_args`. However, since you can't pass your `define_experiments` function as a command line argument, you have to save it in a `.py` file and pass the name of that file as the first argument to `run_experiments.py`.\n",
    "\n",
    "To query the database, we can use function `select_to_dataframe(sql_string)` with a suitable `sql_string`. There are many other powerful functions included for querying and plotting data, but those are covered in `microbench_experiments/example/instructions_data.ipynb`. In **this** notebook we are focusing on the design of the `define_experiment` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_in_jupyter(define_experiment, cmdline_args='--production --no-compile --no-run --no-createdb')\n",
    "select_to_dataframe('select * from data')\n",
    "\n",
    "# run_in_jupyter call above has equivalent command:\n",
    "# [...]/run_experiments.sh myexp.py --production --no-compile --no-run --no-createdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppressing logging output in `run_in_jupyter`\n",
    "\n",
    "If you want to call `run_in_jupyter` without seeing the logs copied to stdout, you can disable the log output by calling `disable_tee_stdout()`. Note that logs will still occur, but the output will **only** go to the log file `output_log.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_tee_stdout()\n",
    "run_in_jupyter(define_experiment, cmdline_args='--production --no-compile --no-run --no-createdb')\n",
    "select_to_dataframe('select * from data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractors: mining data from arbitrary text\n",
    "\n",
    "    ## by default, a field \"XYZ\" will be fetched from each data file using extractor grep_line,\n",
    "    ##      which greps (searches) for a line of the form \"XYZ=[arbitrary string]\\n\"\n",
    "    ##\n",
    "    ## if your field is not stored in that format, you can specify a custom \"extractor\" function,\n",
    "    ##      as we do in our example \"get_maxres\" BELOW, to extract the max resident size\n",
    "    ##      from the 6th space-separated column of the output of the linux \"time\" command\n",
    "    ##\n",
    "    ## also note: each of these fields becomes a replacement token, e.g., {PAPI_L3_TCM}.\n",
    "    ##\n",
    "    ## the following special fields are also defined for you:\n",
    "    ##      {__step}            the number of runs done so far, padded to six digits with leading zeros\n",
    "    ##      {__cmd_run}         your cmd_run string with any tokens replaced appropriately for this run\n",
    "    ##      {__file_data}       the output filename for the current run's data\n",
    "    ##      {__path_data}       the relative path to the output file for the current run's data\n",
    "    ##      {__hostname}        the result of running the hostname command on the machine\n",
    "    ##      {__id}              a unique row ID\n",
    "\n",
    "    ## note: in the following, defaults are \"validator=is_nonempty\" and \"extractor=grep_line\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text output we are *trying* to extract max resident size from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text output we are trying to extract max resident size from in MB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractor that accomplishes this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using** this extractor in `define_experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validators: *checking* extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running multiple trials\n",
    "\n",
    "    ## if you want to perform repeated trials of each experimental configuration, add a run_param called \"__trial\"\n",
    "    ##     and specify a list of trial numbers (as below).\n",
    "    ##\n",
    "    ## (the run_param doesn't *need* to be called __trials exactly, but if it is called __trials exactly,\n",
    "    ##     then extra sanity checks will be performed to verify, for example, that each data point in a graphical plot\n",
    "    ##     represents the average of precisely as many experimental runs as there are entries in the __trials list.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../../tools/data_framework') ; from run_experiment import *\n",
    "\n",
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_compile  (exp_dict, os.getcwd() + '/../../microbench')\n",
    "    set_dir_run      (exp_dict, os.getcwd() + '/../../microbench/bin')\n",
    "    set_cmd_compile  (exp_dict, 'make -j all')\n",
    "\n",
    "    add_run_param    (exp_dict, '__trials', [1, 2, 3])\n",
    "    add_run_param    (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_run      (exp_dict, 'LD_PRELOAD=../../../lib/libjemalloc.so numactl --interleave=all time ./ubench_{DS_TYPENAME}.alloc_new.reclaim_debra.pool_none.out -nwork 1 -nprefill 1 -insdel 5 5 -k 200000 -t 1000')\n",
    "\n",
    "    add_data_field   (exp_dict, 'DS_TYPENAME')\n",
    "    add_data_field   (exp_dict, 'total_throughput')\n",
    "\n",
    "run_in_jupyter(define_experiment, cmdline_args='--production --no-compile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the data (to see the multiple trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_to_dataframe('select * from data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "\n",
    "- validators\n",
    "- extractors\n",
    "- testing mode\n",
    "- plots\n",
    "- pages\n",
    "- special reserved data field names\n",
    "- custom output filename pattern, output path\n",
    "- best-effort automated sanity checks\n",
    "- complete control with custom plot function\n",
    "\n",
    "    ## pattern for output filenames. note 1: these files will be placed in {__dir_data}/. note 2: filenames cannot contain spaces.\n",
    "    set_file_data   ( exp_dict, 'data{__step}.txt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_experiment(exp_dict, args):\n",
    "    set_dir_tools   (exp_dict, os.getcwd() + '/../../tools')      ## path to prereq. tools library\n",
    "    set_dir_compile (exp_dict, os.getcwd() + '/../../microbench') ## working directory for compile\n",
    "    set_dir_run     (exp_dict, os.getcwd() + '/bin')              ## working directory for run\n",
    "    set_dir_data    (exp_dict, os.getcwd() + '/data')             ## path for output files\n",
    "\n",
    "    add_run_param   (exp_dict, '__trials', [1, 2, 3])\n",
    "    add_run_param   (exp_dict, 'DS_TYPENAME', ['brown_ext_ist_lf', 'brown_ext_abtree_lf', 'bronson_pext_bst_occ'])\n",
    "\n",
    "    set_cmd_compile (exp_dict, 'make -j all bin_dir={__dir_run}')\n",
    "    set_cmd_run     (exp_dict, 'LD_PRELOAD=../../../lib/libjemalloc.so numactl --interleave=all time ./ubench_{DS_TYPENAME}.alloc_new.reclaim_debra.pool_none.out -nwork 190 -nprefill 190 -insdel 5 5 -k 2000000 -t 3000 -rq 0 -rqsize 1 -nrq 0')\n",
    "\n",
    "    set_file_data   (exp_dict, 'data_{DS_TYPENAME}.txt')\n",
    "\n",
    "    add_data_field  (exp_dict, 'DS_TYPENAME'      , coltype='TEXT'    , validator=is_run_param('DS_TYPENAME'))\n",
    "    add_data_field  (exp_dict, 'total_throughput' , coltype='INTEGER' , validator=is_positive)\n",
    "\n",
    "    add_plot_set( \\\n",
    "            exp_dict \\\n",
    "          , name='throughput.png' \\\n",
    "          , title='Throughput vs data structure' \\\n",
    "          , x_axis='DS_TYPENAME' \\\n",
    "          , y_axis='total_throughput' \\\n",
    "          , plot_type='bars' \\\n",
    "    )\n",
    "    return exp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594173183435",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}